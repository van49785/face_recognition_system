import os
import math
import random
import base64
from io import BytesIO
from PIL import Image
from datetime import datetime, timedelta 

import cv2
import dlib
import numpy as np
import face_recognition 
import insightface

# === Local imports ===
from app.models.employee import Employee
from app.models.face_training_data import FaceTrainingData

# --------------------------------------------------
# Load dlib predictors & detectors once at module import
# --------------------------------------------------
BASE_DIR = os.path.dirname(os.path.abspath(__file__))  # .../app/services
MODEL_PATH = os.path.abspath(os.path.join(BASE_DIR, "..", "..", "shape_predictor_68_face_landmarks.dat"))
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError("Landmark model not found at %s" % MODEL_PATH)

_shape_predictor = dlib.shape_predictor(MODEL_PATH)
_face_detector = dlib.get_frontal_face_detector()
_face_analyzer = insightface.app.FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])
_face_analyzer.prepare(ctx_id=0)

# --------------------------------------------------
# Utility ‚Äì extract 68 landmarks as (68, 2) numpy array
# --------------------------------------------------

def _extract_landmarks(image_bgr: np.ndarray):
    """Return landmarks (68,2) or None if not exactly one face."""
    gray = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2GRAY)
    rects = _face_detector(gray, 1)
    if len(rects) != 1:
        return None
    shape = _shape_predictor(gray, rects[0])
    coords = np.zeros((68, 2), dtype="float")
    for i in range(68):
        coords[i] = (shape.part(i).x, shape.part(i).y)
    return coords

# --------------------------------------------------
# LivenessChecker ‚Äì unchanged internal logic but expects (68,2)
# --------------------------------------------------
class LivenessChecker:
    def __init__(self):
        # self.actions = ["blink", "smile", "turn_left", "turn_right", "nod"]
        self.actions = ['blink', 'smile'] # V·∫´n gi·ªØ ƒë·ªÉ s·ª≠ d·ª•ng c√°c h√†m con

    def detect_blink(self, landmarks):
        def ear(eye):
            A = np.linalg.norm(eye[1] - eye[5])
            B = np.linalg.norm(eye[2] - eye[4])
            C = np.linalg.norm(eye[0] - eye[3])
            return (A + B) / (2.0 * C)
        left, right = landmarks[36:42], landmarks[42:48]
        return (ear(left) + ear(right)) / 2 < 0.25

    def detect_smile(self, landmarks):
        mouth = landmarks[48:68]
        width = np.linalg.norm(mouth[0] - mouth[6])
        height = np.linalg.norm(mouth[3] - mouth[9])
        ratio = (width / height) if height else 0
        return ratio > 3.0

    def detect_head_pose(self, landmarks):
        # H√†m n√†y kh√¥ng ƒë∆∞·ª£c s·ª≠ d·ª•ng tr·ª±c ti·∫øp trong liveness check hi·ªán t·∫°i, ch·ªâ d√πng ƒë·ªÉ minh h·ªça
        # v√† s·∫Ω kh√¥ng thay ƒë·ªïi ƒë·ªÉ gi·ªØ nguy√™n t√™n h√†m
        nose_tip, chin = landmarks[30], landmarks[8]
        left_eye = landmarks[36:42].mean(axis=0)
        right_eye = landmarks[42:48].mean(axis=0)
        angle_deg = math.degrees(math.atan2(right_eye[1]-left_eye[1], right_eye[0]-left_eye[0]))
        if angle_deg > 15:
            return "turn_left"
        if angle_deg < -15:
            return "turn_right"
        face_center_x = (landmarks[0][0] + landmarks[16][0]) / 2
        if abs(nose_tip[0] - face_center_x) < 5:
            return "nod"
        return "center"

# --------------------------------------------------
# Global Liveness Session Management
# --------------------------------------------------
# L∆∞u tr·ªØ tr·∫°ng th√°i liveness cho m·ªói phi√™n (session_id)
# M·ªói phi√™n s·∫Ω l∆∞u tr·ªØ m·ªôt s·ªë khung h√¨nh g·∫ßn nh·∫•t v√† tr·∫°ng th√°i liveness
LIVENESS_SESSIONS = {}
SESSION_TIMEOUT_SECONDS = 5 # Th·ªùi gian ch·ªù t·ªëi ƒëa cho m·ªôt phi√™n liveness (v√≠ d·ª•: 5 gi√¢y)
MAX_FRAMES_PER_SESSION = 30 # S·ªë l∆∞·ª£ng khung h√¨nh t·ªëi ƒëa ƒë∆∞·ª£c l∆∞u tr·ªØ cho m·ªói phi√™n

# --------------------------------------------------
# FacialRecognitionService
# --------------------------------------------------
class FacialRecognitionService:
    liveness_checker = LivenessChecker()

    # ---------- Decoders / preprocess helpers ----------
    @staticmethod
    def _bytes_to_image(image_bytes: bytes):
        arr = np.frombuffer(image_bytes, np.uint8)
        return cv2.imdecode(arr, cv2.IMREAD_COLOR)

    @staticmethod
    def preprocess_image(file_like):
        """Validate lighting & single face."""
        img = FacialRecognitionService._bytes_to_image(file_like.read())
        if img is None:
            return False, "Invalid image", None
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        if gray.mean() < 50:
            return False, "Insufficient lighting, please take photo in brighter place", None
        
        # S·ª≠ d·ª•ng InsightFace ƒë·ªÉ ph√°t hi·ªán khu√¥n m·∫∑t thay v√¨ face_recognition
        faces = _face_analyzer.get(img)
        if not faces or len(faces) != 1:
            return False, "Please ensure exactly one face is visible", None
        
        return True, "OK", img

    @staticmethod
    def decode_base64_image(b64: str):
        try:
            if "," in b64:
                b64 = b64.split(",", 1)[1]
            return FacialRecognitionService._bytes_to_image(base64.b64decode(b64))
        except (ValueError, TypeError, base64.binascii.Error) as e:
            return None

    # ---------- Quality / pose ----------
    @staticmethod
    def calculate_image_quality(img):
        """T√≠nh to√°n ƒëi·ªÉm ch·∫•t l∆∞·ª£ng ·∫£nh d·ª±a tr√™n ƒë·ªô s·∫Øc n√©t v√† ƒë·ªô t∆∞∆°ng ph·∫£n."""
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        sharp = cv2.Laplacian(gray, cv2.CV_64F).var()
        contrast = gray.std()
        # C√¥ng th·ª©c heuristic, c√≥ th·ªÉ c·∫ßn hi·ªáu ch·ªânh th√™m
        return min(100, (sharp/100)*50 + (contrast/50)*50)

    @staticmethod
    def _get_pose_from_angles(pitch, yaw, roll):
        """
        Chuy·ªÉn ƒë·ªïi c√°c g√≥c pitch, yaw, roll th√†nh c√°c lo·∫°i pose ƒë·ªãnh t√≠nh.
        D·ª±a tr√™n ng∆∞·ª°ng √°ng ch·ª´ng, c·∫ßn ki·ªÉm tra v√† tinh ch·ªânh.
        """
        # Ng∆∞·ª°ng (c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
        yaw_threshold = 15 # degrees for left/right
        pitch_threshold = 15 # degrees for up/down

        if yaw > yaw_threshold:
            return "right" # quay sang ph·∫£i
        elif yaw < -yaw_threshold:
            return "left" # quay sang tr√°i
        elif pitch > pitch_threshold:
            return "down" # nh√¨n xu·ªëng
        elif pitch < -pitch_threshold:
            return "up" # nh√¨n l√™n
        else:
            return "front" # ch√≠nh di·ªán

    @staticmethod
    def detect_face_pose(img, faces): # Thay ƒë·ªïi tham s·ªë, nh·∫≠n directly faces object t·ª´ InsightFace
        """
        X√°c ƒë·ªãnh lo·∫°i pose khu√¥n m·∫∑t d·ª±a tr√™n th√¥ng tin t·ª´ InsightFace.
        """
        if not faces or len(faces) != 1:
            return "unknown", 0.0 # Kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c ho·∫∑c nhi·ªÅu h∆°n 1 khu√¥n m·∫∑t

        face = faces[0]
        # InsightFace cung c·∫•p c√°c g√≥c pitch, yaw, roll
        pitch = face.pose[0] # G√≥c nghi√™ng l√™n/xu·ªëng
        yaw = face.pose[1]   # G√≥c quay tr√°i/ph·∫£i
        roll = face.pose[2]  # G√≥c nghi√™ng ƒë·∫ßu

        pose_type = FacialRecognitionService._get_pose_from_angles(pitch, yaw, roll)
        
        # ƒê·ªô tin c·∫≠y (c√≥ th·ªÉ l√† 1 - kho·∫£ng c√°ch t·ª´ 0,0,0 n·∫øu mu·ªën)
        # Hi·ªán t·∫°i tr·∫£ v·ªÅ 0.0 v√¨ h√†m g·ªëc c≈©ng tr·∫£ v·ªÅ 0.0 cho ƒë·ªô tin c·∫≠y.
        return pose_type, 0.0 

    # ---------- NEW: Helper function to check and update training completion ----------
    @staticmethod
    def check_and_update_training_completion(employee_id):
        """Ki·ªÉm tra v√† c·∫≠p nh·∫≠t tr·∫°ng th√°i training completion sau khi th√™m pose m·ªõi"""
        try:
            employee = Employee.query.filter_by(employee_id=employee_id).first()
            if not employee:
                print(f"‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y nh√¢n vi√™n v·ªõi ID: {employee_id}")
                return False
            
            # Ki·ªÉm tra ƒë·ªß pose v√† ch·∫•t l∆∞·ª£ng
            has_sufficient, missing_poses = FaceTrainingData.has_sufficient_poses(
                employee_id, min_quality=35
            )
            
            print(f"üîç Ki·ªÉm tra training completion cho {employee_id}:")
            print(f"   - ƒê·ªß pose: {has_sufficient}")
            print(f"   - Pose c√≤n thi·∫øu: {missing_poses}")
            print(f"   - Training completed hi·ªán t·∫°i: {employee.face_training_completed}")
            
            if has_sufficient and not employee.face_training_completed:
                print(f"‚úÖ C·∫≠p nh·∫≠t training completion cho {employee_id}")
                employee.complete_face_training()
                return True
            
            return False
            
        except Exception as e:
            print(f"‚ùå L·ªói khi c·∫≠p nh·∫≠t training completion cho {employee_id}: {e}")
            return False

    # ---------- Encoding with metadata (FIXED) ----------
    @staticmethod
    def generate_face_encoding_with_metadata(file_like, pose_type=None, employee_id=None):
        """
        Ti·ªÅn x·ª≠ l√Ω ·∫£nh, ph√°t hi·ªán khu√¥n m·∫∑t v√† sinh ra vector encoding (512D, ArcFace).
        FIXED: Th√™m tham s·ªë employee_id v√† logic auto-update training completion
        
        Args:
            file_like: File object ch·ª©a ·∫£nh
            pose_type: Lo·∫°i pose (ƒë·ªÉ t∆∞∆°ng th√≠ch ng∆∞·ª£c)
            employee_id: ID nh√¢n vi√™n (ƒë·ªÉ c·∫≠p nh·∫≠t training completion)
            
        Returns:
            (th√†nh c√¥ng, th√¥ng b√°o, vector bytes, metadata dict)
        """
        # 1. ƒê·ªçc ·∫£nh v√† ki·ªÉm tra ƒëi·ªÅu ki·ªán √°nh s√°ng
        img = FacialRecognitionService._bytes_to_image(file_like.read())
        if img is None:
            return False, "Invalid image", None, None

        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        if gray.mean() < 50:
            return False, "Insufficient lighting, please take photo in brighter place", None, None

        # 2. Ph√°t hi·ªán khu√¥n m·∫∑t v√† sinh encoding b·∫±ng InsightFace
        faces = _face_analyzer.get(img)
        if not faces or len(faces) != 1:
            return False, "Please ensure exactly one face is visible", None, None

        face = faces[0]
        embedding = face.embedding.astype(np.float32).tobytes()  # L∆∞u vector 512D d·∫°ng bytes

        # 3. T√≠nh ch·∫•t l∆∞·ª£ng ·∫£nh ƒë·∫ßu v√†o
        quality = FacialRecognitionService.calculate_image_quality(img)

        # 4. T·ª± ƒë·ªông x√°c ƒë·ªãnh pose n·∫øu ch∆∞a cung c·∫•p
        # S·ª≠ d·ª•ng detect_face_pose m·ªõi ƒë·ªÉ x√°c ƒë·ªãnh pose
        auto_detected_pose, _ = FacialRecognitionService.detect_face_pose(img, faces) 
        if not pose_type:
            pose_type = auto_detected_pose

        metadata = {
            "pose_type": pose_type,
            "image_quality_score": quality
        }

        # 5. NEW: Ki·ªÉm tra v√† c·∫≠p nh·∫≠t training completion sau khi t·∫°o encoding th√†nh c√¥ng
        if employee_id:
            print(f"üéØ ƒê√£ t·∫°o encoding th√†nh c√¥ng cho {employee_id}, pose: {pose_type}")
            # Ch·∫°y async ƒë·ªÉ kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn lu·ªìng ch√≠nh
            try:
                FacialRecognitionService.check_and_update_training_completion(employee_id)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi c·∫≠p nh·∫≠t training completion: {e}")
                # Kh√¥ng return False v√¨ encoding ƒë√£ t·∫°o th√†nh c√¥ng

        return True, "Success", embedding, metadata

    # ---------- NEW: Batch update function for existing data ----------
    @staticmethod
    def fix_existing_training_data():
        """
        Kh·∫Øc ph·ª•c d·ªØ li·ªáu training cho c√°c nh√¢n vi√™n ƒë√£ train ƒë·ªß nh∆∞ng ch∆∞a ƒë∆∞·ª£c ƒë√°nh d·∫•u completed
        """
        print("üîß B·∫Øt ƒë·∫ßu kh·∫Øc ph·ª•c d·ªØ li·ªáu training hi·ªán c√≥...")
        
        try:
            employees = Employee.query.all()
            updated_count = 0
            
            for emp in employees:
                # Ki·ªÉm tra s·ªë pose hi·ªán c√≥
                pose_count = FaceTrainingData.get_employee_pose_count(emp.employee_id)
                has_sufficient, missing_poses = FaceTrainingData.has_sufficient_poses(
                    emp.employee_id, min_quality=35
                )
                
                print(f"üìä Nh√¢n vi√™n {emp.employee_id} ({emp.full_name}):")
                print(f"   - S·ªë pose: {pose_count}")
                print(f"   - ƒê·ªß pose: {has_sufficient}")
                print(f"   - Training completed: {emp.face_training_completed}")
                
                if has_sufficient and not emp.face_training_completed:
                    print(f"   ‚úÖ C·∫≠p nh·∫≠t training status...")
                    emp.complete_face_training()
                    updated_count += 1
                elif emp.face_training_completed and not has_sufficient:
                    print(f"   ‚ö†Ô∏è D·ªØ li·ªáu kh√¥ng nh·∫•t qu√°n: ƒë√£ completed nh∆∞ng kh√¥ng ƒë·ªß pose")
                
                print()
            
            print(f"üéâ Ho√†n th√†nh! ƒê√£ c·∫≠p nh·∫≠t {updated_count} nh√¢n vi√™n.")
            return updated_count
            
        except Exception as e:
            print(f"‚ùå L·ªói khi kh·∫Øc ph·ª•c d·ªØ li·ªáu: {e}")
            return 0

    # ---------- Liveness (Enhanced for sequence analysis) ----------
    @staticmethod
    def detect_liveness(img, session_id: str):
        # 1. Qu·∫£n l√Ω v√† d·ªçn d·∫πp c√°c phi√™n c≈©
        current_time = datetime.now()
        for sid in list(LIVENESS_SESSIONS.keys()):
            if (current_time - LIVENESS_SESSIONS[sid]['last_update']).total_seconds() > SESSION_TIMEOUT_SECONDS:
                del LIVENESS_SESSIONS[sid]

        # 2. Kh·ªüi t·∫°o/C·∫≠p nh·∫≠t phi√™n hi·ªán t·∫°i
        if session_id not in LIVENESS_SESSIONS:
            LIVENESS_SESSIONS[session_id] = {
                'frames_data': [], # L∆∞u tr·ªØ landmarks v√† timestamp
                'liveness_passed': False,
                'last_update': current_time,
                'blink_detected_in_session': False,
                'smile_detected_in_session': False
            }

        session_data = LIVENESS_SESSIONS[session_id]
        session_data['last_update'] = current_time

        # 3. Ki·ªÉm tra √°nh s√°ng
        if np.std(cv2.cvtColor(img,cv2.COLOR_BGR2GRAY))<10:
            # N·∫øu √°nh s√°ng y·∫øu, x√≥a phi√™n v√† b√°o l·ªói
            if session_id in LIVENESS_SESSIONS:
                del LIVENESS_SESSIONS[session_id]
            return False,"The lighting is too dim, please try again in a brighter location."

        # 4. Tr√≠ch xu·∫•t landmarks v√† ki·ªÉm tra ph√°t hi·ªán khu√¥n m·∫∑t
        landmarks = _extract_landmarks(img) # V·∫´n d√πng dlib landmarks cho liveness
        if landmarks is None:
            return False,"Unable to detect the face, please look directly into the camera."

        # 5. Th√™m d·ªØ li·ªáu khung h√¨nh hi·ªán t·∫°i v√†o phi√™n
        session_data['frames_data'].append({
            'timestamp': current_time,
            'landmarks': landmarks
        })

        # 6. D·ªçn d·∫πp khung h√¨nh c≈© h∆°n 5 gi√¢y
        session_data['frames_data'] = [
            f_data for f_data in session_data['frames_data']
            if (current_time - f_data['timestamp']).total_seconds() <= SESSION_TIMEOUT_SECONDS
        ]

        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng khung h√¨nh ƒë·ªÉ tr√°nh tr√†n b·ªô nh·ªõ
        if len(session_data['frames_data']) > MAX_FRAMES_PER_SESSION:
            session_data['frames_data'].pop(0)

        # 7. Ph√¢n t√≠ch chu·ªói khung h√¨nh ƒë·ªÉ ph√°t hi·ªán liveness
        # Ch·ªâ c·∫ßn m·ªôt c√°i ch·ªõp m·∫Øt HO·∫∂C m·ªôt n·ª• c∆∞·ªùi
        if not session_data['liveness_passed']:
            checker = FacialRecognitionService.liveness_checker

            for frame_data in session_data['frames_data']:
                if not session_data['blink_detected_in_session'] and checker.detect_blink(frame_data['landmarks']):
                    session_data['blink_detected_in_session'] = True
                    # Kh√¥ng break ·ªü ƒë√¢y ƒë·ªÉ v·∫´n c√≥ th·ªÉ t√¨m th·∫•y n·ª• c∆∞·ªùi trong c√πng phi√™n
            
            for frame_data in session_data['frames_data']:
                if not session_data['smile_detected_in_session'] and checker.detect_smile(frame_data['landmarks']):
                    session_data['smile_detected_in_session'] = True
                    # Kh√¥ng break ·ªü ƒë√¢y ƒë·ªÉ v·∫´n c√≥ th·ªÉ t√¨m th·∫•y ch·ªõp m·∫Øt trong c√πng phi√™n

            # Quy·∫øt ƒë·ªãnh liveness: ƒë√£ ph√°t hi·ªán ch·ªõp m·∫Øt HO·∫∂C c∆∞·ªùi
            if session_data['blink_detected_in_session'] or session_data['smile_detected_in_session']:
                session_data['liveness_passed'] = True
                return True, "Ki·ªÉm tra s·ª± s·ªëng th√†nh c√¥ng."
            else:
                # N·∫øu h·∫øt 5 gi√¢y m√† kh√¥ng ph√°t hi·ªán, coi l√† th·∫•t b·∫°i
                # ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 1 khung h√¨nh trong session_data['frames_data'] tr∆∞·ªõc khi truy c·∫≠p
                if session_data['frames_data'] and (current_time - session_data['frames_data'][0]['timestamp']).total_seconds() > SESSION_TIMEOUT_SECONDS:
                    del LIVENESS_SESSIONS[session_id] # X√≥a phi√™n
                    return False, "No liveness action detected. Please try again and perform a blink or smile."
                return False, "Checking for liveness. Please keep your face clear and blink or smile."
        
        # N·∫øu liveness ƒë√£ ƒë∆∞·ª£c x√°c nh·∫≠n trong phi√™n n√†y
        return True, "Liveness check successful."

    # ---------- Recognition ----------
    @staticmethod
    def recognize_face_with_multiple_encodings(img):
        """
        Nh·∫≠n di·ªán khu√¥n m·∫∑t b·∫±ng InsightFace (ArcFace), ƒë√£ fix chu·∫©n h√≥a vector v√† log chi ti·∫øt.
        B√¢y gi·ªù so s√°nh v·ªõi T·∫§T C·∫¢ c√°c pose ƒë√£ train c·ªßa nh√¢n vi√™n.
        """
        # 1. Ph√°t hi·ªán khu√¥n m·∫∑t v√† l·∫•y embedding
        faces = _face_analyzer.get(img)
        if not faces or len(faces) != 1:
            return False, "Image must contain exactly one face", None

        # 2. Chu·∫©n h√≥a vector query
        query_embedding = faces[0].embedding.astype(np.float32)
        query_embedding = query_embedding / np.linalg.norm(query_embedding) # Chu·∫©n h√≥a L2

        # 3. L·∫•y danh s√°ch nh√¢n vi√™n
        # Ch·ªâ l·∫•y nh√¢n vi√™n ƒëang active v√† ƒë√£ ho√†n t·∫•t training
        employees = Employee.query.filter_by(status=True, face_training_completed=True).all()
        if not employees:
            return False, "No employees have completed face training yet", None

        best_match = {
            "distance": float("inf"),
            "employee": None
        }

        for emp in employees:
            training_data_list = FaceTrainingData.get_employee_encodings(emp.employee_id)
            if not training_data_list: # B·ªè qua n·∫øu kh√¥ng c√≥ d·ªØ li·ªáu training
                continue

            for d in training_data_list:
                try:
                    stored_embedding = np.frombuffer(d.face_encoding, dtype=np.float32)
                    if stored_embedding.shape[0] != 512: # ƒê·∫£m b·∫£o ƒë√∫ng k√≠ch th∆∞·ªõc
                        print(f"L·ªói: K√≠ch th∆∞·ªõc embedding c·ªßa {emp.employee_id} kh√¥ng ƒë√∫ng (ph·∫£i l√† 512).")
                        continue
                    stored_embedding = stored_embedding / np.linalg.norm(stored_embedding) # Chu·∫©n h√≥a L2
                except Exception as e:
                    print(f"‚ùå L·ªói gi·∫£i m√£ ho·∫∑c chu·∫©n h√≥a vector t·ª´ DB c·ªßa {emp.employee_id}: {e}")
                    continue

                # Ch·ªâ l·ªçc d·ª±a tr√™n image_quality_score, kh√¥ng l·ªçc pose_type n·ªØa
                # L√Ω do: InsightFace m·∫°nh m·∫Ω h∆°n v·ªõi c√°c bi·∫øn th·ªÉ pose, s·ª≠ d·ª•ng t·∫•t c·∫£ training data
                # ƒë·ªÉ t√¨m match t·ªët nh·∫•t s·∫Ω tƒÉng robustness.
                if d.image_quality_score is not None and d.image_quality_score < 35:
                    continue

                # T√≠nh ƒë·ªô t∆∞∆°ng ƒë·ªìng cosine (cosine distance = 1 - cosine similarity)
                cos_sim = np.dot(query_embedding, stored_embedding)
                pseudo_dist = 1 - cos_sim

                if pseudo_dist < best_match["distance"]:
                    best_match["distance"] = pseudo_dist
                    best_match["employee"] = emp

        # 4. So s√°nh v·ªõi ng∆∞·ª°ng
        THRESHOLD = 0.35  # Ng∆∞·ª°ng c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh sau khi th·ª≠ nghi·ªám th·ª±c t·∫ø.
                          # Kho·∫£ng c√°ch nh·ªè h∆°n -> t∆∞∆°ng ƒë·ªìng cao h∆°n.
        if best_match["employee"] and best_match["distance"] < THRESHOLD:
            print(f"‚úÖ T√¨m th·∫•y kh·ªõp: {best_match['employee'].employee_id} ‚Äì Kho·∫£ng c√°ch: {best_match['distance']:.4f}")
            return True, "Face recognized successfully", best_match["employee"]
        else:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y nh√¢n vi√™n ph√π h·ª£p. Kho·∫£ng c√°ch t·ªët nh·∫•t: {best_match['distance']:.4f}")
            return False, "Face does not match any registered employee", None


    @staticmethod
    def recognize_face_with_liveness(img, session_id: str): # B·ªï sung tham s·ªë session_id
        live_ok, live_msg = FacialRecognitionService.detect_liveness(img, session_id)
        if not live_ok:
            return False, live_msg, None
        
        # N·∫øu liveness ƒë√£ passed, ti·∫øp t·ª•c nh·∫≠n di·ªán
        return FacialRecognitionService.recognize_face_with_multiple_encodings(img)

    # ---------- Misc ----------
    @staticmethod
    def get_required_poses():
        return ["front","left","right","up","down"]